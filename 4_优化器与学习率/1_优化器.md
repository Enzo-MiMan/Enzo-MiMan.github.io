

# 优化器 <!-- {docsify-ignore} -->



![image-20231012231744691](https://p.ipic.vip/8zbxe9.png)

## 1、梯度下降法

传统权重更新算法为最常见、最简单的一种参数更新策略。

（1）基本思想 ：先设定一个学习率 $\eta$，参数沿梯度的反方向移动。假设需要更新的参数为$w$，梯度为$g$，则其更新策略可表示为：
$$w \leftarrow w - \eta * g$$

<br />

（2）梯度下降法有三种不同的形式：

- BGD（Batch Gradient Descent）：批量梯度下降，每次参数更新使用 <mark>所有样本 </mark>
- SGD（Stochastic Gradient Descent）：随机梯度下降，每次参数更新只使用 <mark>1个样本</mark>
- MBGD（Mini-Batch Gradient Descent）：小批量梯度下降，每次参数更新使用 <mark>小部分数据样本</mark>（mini_batch）
		

这三个优化算法在训练的时候虽然所采用的的数据量不同，但是他们在进行参数优化的时候，采用的方法是相同的：

$\qquad$ step 1 ： $g = \frac{\partial loss}{\partial w}$

$\qquad$ step 2 ：求梯度的平均值

$\qquad$ step 3 ： 更新权重 ：$w \leftarrow w - \eta * g$



<br />

（3）优缺点

优点：

- 算法简洁，当学习率取值恰当时，可以收敛到 全局最优点(凸函数) 或 局部最优点(非凸函数)。

缺点：
- 对超参数学习率比较敏感：过小导致收敛速度过慢，过大又越过极值点

- 学习率除了敏感，有时还会因其在迭代过程中保持不变，很容易造成算法被卡在鞍点的位置。

- 在较平坦的区域，由于梯度接近于0，优化算法会因误判，在还未到达极值点时，就提前结束迭代，陷入局部极小值。

  

  <img src="https://p.ipic.vip/g57zuh.png" alt="鞍点位置" style="zoom:45%;" />



在训练的时候一般都是使用小批量梯度下降算法，即选择部分数据进行训练，这里把这三种算法统称为<mark>传统梯度下降法</mark> 。

而更优的优化算法从<mark>梯度方面</mark> 和 <mark>学习率方面</mark>对参数更新方式进行优化

<br />





### 1.1 一维梯度下降法

我们以 目标函数（损失函数） $f(x)= x^2$  为例来看一看梯度下降是如何工作的（这里 $x$ 为参数）。

迭代方法为： 
$$x \leftarrow x - \eta * g = x - \eta * \frac{\partial loss}{\partial x}$$ 



虽然我们知道最小化 $f(x)$ 的解为 $x=0$ ，这里依然使用如下代码来观察 $x$ 是如何迭代的

这里$x$为模型参数，使用 $x = 10$ 作为初始值，并设 学习率 $\eta = 0.2$，使用梯度下降法 对$x$ 迭代10次

```python
import numpy as np
import matplotlib.pyplot as plt

x = 10
lr = 0.2
result = [x]

for i in range(10):
    x -= lr * 2 * x
    result.append(x)
    
f_line = np.arange(-10, 10, 0.1)
plt.plot(f_line, [x * x for x in f_line])
plt.plot(result, [x * x for x in result], '-o')
plt.title('learning rate = {}'.format(lr))
plt.xlabel('x')
plt.ylabel('f(x)')
plt.show()
```
<img src="https://p.ipic.vip/c4sk08.png" alt="image-20231012232200188" style="zoom:33%;" />

大家可以尝试使用不同的学习率进行训练，会得到如下结果：

![在这里插入图片描述](https://p.ipic.vip/k2ipej.png)

- 如果使用的学习率太小，将导致$x$的更新非常缓慢，需要更多的迭代。
- 相反，当使用过大的学习率，$x$的迭代不能保证降低$f(x)$ 的值，例如，当学习率为$\eta=1.1$时，$x$超出了最优解 $x=0$，并逐渐发散

<br />

<br />

### 1.2 多维梯度下降法

在对一元梯度下降有了了解之后，下面看看多元梯度下降，即考虑 $X=[x_1, x_2, \cdots x_d]^T$ 的情况。

多元损失函数，它的梯度也是多元的，是一个由d个偏导数组成的向量:
$$\nabla f(X) = [\frac{\partial f_x}{\partial x_1}, \frac{\partial f_x}{\partial x_2}, \cdots, \frac{\partial f_x}{\partial x_d}]^T$$

然后选择合适的学率进行梯度下降：
$$x_i \leftarrow x_i - \eta * \nabla f(X)$$

<br />

下面通过代码可视化它的参数更新过程。构造一个目标函数 $f(X)=x_1^2+2x_2^2$，并有二维向量 $X = [x_1, x_2]$ 作为输入，标量作为输出。 

损失函数的梯度为 $\nabla f(x) = [2x_1,4x_2]^T$ 。使用梯度下降法，观察$x_1, x_2$从初始位置[-5, -2] 的更新轨迹。


```python
import numpy as np
import matplotlib.pyplot as plt


def loss_func(x1, x2):  # 定义目标函数
    return x1 ** 2 + 2 * x2 ** 2


x1, x2 = -5, -2
eta = 0.4
num_epochs = 20
result = [(x1, x2)]

for epoch in range(num_epochs):
    gd1 = 2 * x1
    gd2 = 4 * x2

    x1 -= eta * gd1
    x2 -= eta * gd2

    result.append((x1, x2))

# print('x1:', result1)
# print('\n x2:', result2)

plt.figure(figsize=(8, 4))
plt.plot(*zip(*result), '-o', color='#ff7f0e')
x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))
plt.contour(x1, x2, loss_func(x1, x2), colors='#1f77b4')
plt.title('learning rate = {}'.format(eta))
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()
```

<img src="https://p.ipic.vip/1b3b9i.png" alt="image-20231012232550982" style="zoom:33%;" />



<br />

<br />

## 2、动量(Momentum)



<img src="https://p.ipic.vip/5w2e5v.gif" alt="图1" style="zoom:50%;" />



思想：让参数的更新具有惯性， 每一步更新 都是由前面梯度的累积 $v$ 和当前点梯度 $g$ 组合而成

公式 ：

<mark>$\qquad 累计梯度更新：v \leftarrow \alpha v + (1 - \alpha)g.  \qquad 其中，\alpha 为动量参数，v 为累计梯度，v 为当前梯度，\eta  为学习率$</mark>

<mark>$\qquad 梯度更新：w \leftarrow w - \eta*v$</mark>

优点：

1、加快收敛能帮助参数在 正确的方向上加速前进

2、他可以帮助跳出局部最小值

<img src="https://p.ipic.vip/m4ppgm.png" alt="image-20231013154054695" style="zoom:45%;" />

<br />

**实验一 ：**

损失函数： $f(x)=0.1x_1^2+2x_2^2$ ，$x_1, x_2$初始值分别为-5， -2， 学习率设为 0.4，我们使用<font color="red"> 不带动量的传统梯度下降法 </font>，观察其下降过程

预期分析 ： 因为 $x_1$ 和 $x_2$ 的系数分别是 0.1 和 2， 这就使得 $x_1$ 和 $x_2$ 的梯度值相差一个量级，如果使用相同的学习率，$x_2$ 的更新幅度会较$x_1$ 的更大些。


```python
import numpy as np
import matplotlib.pyplot as plt

def loss_func(x1, x2): #定义目标函数
    return 0.1 * x1 ** 2 + 2 * x2 ** 2

x1, x2 = -5, -2
eta = 0.4
num_epochs = 20
result = [(x1, x2)]

for epoch in range(num_epochs):
    gd1 = 0.2 * x1
    gd2 = 4 * x2
    
    x1 -= eta * gd1
    x2 -= eta * gd2
    
    result.append((x1, x2))

plt.plot(*zip(*result), '-o', color='#ff7f0e')
x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))
plt.contour(x1, x2, loss_func(x1, x2), colors='#1f77b4')
plt.title('learning rate = {}'.format(eta))
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()
```

<img src="https://p.ipic.vip/b7tuv7.png" alt="image-20231012233204851" style="zoom:33%;" />

结果分析：与预想一致，使用相同的学习率，$x_2$ 的更新幅度会较$x_1$ 的更大些，变化快得多，而 $x_1$ 收敛速度太慢



<br />

**实验二 ：**

依然使用不带动量的梯度下降算法，<mark>将学习率设置为 0.6</mark>。

更新过程 ： $x_1 \leftarrow x_1 - 0.06 x_1$，$\quad x_2 \leftarrow  x_2 - 2.4 x_2$



更新过程如下图：

<img src="https://p.ipic.vip/jstet7.png" alt="image-20231012233257601" style="zoom:33%;" />

这时，我们会陷入一个两难的选择：

- 如果选择较小的学习率。$x_1$ 收敛缓慢
- 如果选择较大的学习率，$x_1$方向会收敛很快，但在 $x_2$ 方向不会收敛



<br />

<br />

**实验三：**

我们使用<mark>带动量的 梯度下降法</mark>，将历史的梯度考虑在内： 动量参数设为 0.5，将学习率设置为 0.4

$累计梯度更新：v \leftarrow \alpha v + (1 - \alpha)g$ ， $\quad 权重更新： x \leftarrow x - \eta*v$

```python
import numpy as np
import matplotlib.pyplot as plt


def loss_func(x1, x2): #定义目标函数
    return 0.1 * x1 ** 2 + 2 * x2 ** 2


x1, x2 = -5, -2
v1, v2 = 0, 0
eta, alpha = 0.4, 0.5
num_epochs = 20
result = [(x1, x2)]

for epoch in range(num_epochs):
    v1 = alpha * v1 + (1 - alpha) * (0.2 * x1)
    v2 = alpha * v2 + (1 - alpha) * (4 * x2)
    
    x1 -= eta * v1
    x2 -= eta * v2
    
    result.append((x1, x2))

plt.plot(*zip(*result), '-o', color='#ff7f0e')
x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))
plt.contour(x1, x2, loss_func(x1, x2), colors='#1f77b4')
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()
```
<img src="https://p.ipic.vip/qa7ds4.png" alt="image-20231012233459525" style="zoom:33%;" />



即使我们将学习率设置为0.6， $x_2$ 的梯度也不会发散了

<img src="https://p.ipic.vip/j0b1s9.png" alt="image-20231012233538991" style="zoom:33%;" />

<br />

<br />




## 3、Adagrad
Adagrad优化算法 被称为<mark>自适应学习率优化算法</mark>，

之前我们讲的随机梯度下降法，对所有的参数，都是使用相同的、固定的学习率进行优化的，但是不同的参数的梯度差异可能很大，使用相同的学习率，效果不会很好



> 举例：
>
> 假设损失函数是 $f(x)=x_1^2+10x_2^2$， $x$ 和$y$ 的初值分别为 $x_1=40, x_2=20$ 
>
> (通过观察，我们即可知道，$x_1=0, x_2=0$ 就是两个参数的极值点 )
>
> $\rightarrow  \frac{\partial loss}{\partial x_1}=80，\frac{\partial loss}{\partial x_2}=400  \qquad  \rightarrow$
> $x_1$将要移动的幅度 小于$x_2$将移动的幅度
>
> 而 $x_1$ 距离离极值点 $x_1=0$ 是较远的，所以，我们使用梯度下降法，效果并不会好
> 

<font color="red"> Adagrad  思想：对于不同参数，设置不同的学习率 </font>

方法：对于每个参数，初始化一个 累计平方梯度 $r=0$，然后每次将该参数的梯度平方求和累加到这个变量 $r$ 上：
$$r \leftarrow r + g^2$$

然后，在更新这个参数的时候，学习率就变为：
$$ \frac{\eta}{\sqrt {r + \delta}}$$

权重更新：
$$w  \leftarrow w - \frac{\eta}{\sqrt {r + \delta}}*g$$

其中，$g$为梯度；$r$为累积平方梯度(初始为0) ；$\eta$为学习率；$\delta$为小参数，避免分母为0，一般取值为$10^{-10}$

这样，不同的参数由于梯度不同，他们对应的$r$大小也就不同，所以学习率也就不同，这也就实现了自适应的学习率。

总结： Adagrad 的核心想法就是，如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡，而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够更快地更新，这就是Adagrad算法加快深层神经网络的训练速度的核心。

<br />

<br />

## 4、RMSProp
RMSProp：Root Mean Square Propagation 均方根传播

RMSProp 是在 adagrad 的基础上，进一步在学习率的方向上优化

累计平方梯度：$r \leftarrow \lambda r + (1 - \lambda)g^2$

参数更新：$w \leftarrow w - \frac{\eta}{\sqrt {r + \delta}}*g$

其中，$g$为梯度，$r$为累积平方梯度(初始为0) ，$\lambda$为衰减系数，$\eta$为学习率，$\delta$为小参数(避免分母为0)

<br />

<br />

## 5、Adam 
在Grandient Descent 的基础上，做了如下几个方面的改进：

1、梯度方面增加了momentum，使用累积梯度： $v \leftarrow \alpha v + (1 - \alpha)g$

2、同 RMSProp 优化算法一样, 对学习率进行优化，使用累积平方梯度：$r \leftarrow \lambda r + (1 - \lambda)g^2$

3、偏差纠正：$\hat{v} = \frac{v}{1 - \alpha^t}$， $\hat{r} = \frac{r}{1 - \lambda^t}$



再如上3点改进的基础上，权重更新：$w \leftarrow w - \frac{\eta}{\sqrt {\hat{r} + \delta}}*\hat{v}$


> **为啥要偏差纠正**
>
> 第1次更新时，$v_1 \leftarrow \alpha v_0+(1- \alpha)g_1$，由于 $v_0$ 的初始是0，且$\alpha$值一般会设置为接近于1，因此$t$较小时，$v$的值是偏向于0的

```python
def adam(learning_rate, beta1, beta2, epsilon, var, grad, m, v, t):
    m = beta1 * m + (1 - beta1) * grad
    v = beta2 * v + (1 - beta2) * grad * grad
    m_hat = m / (1 - beta1 ** t)
    v_hat = v / (1 - beta2 ** t)
    var = var - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
    return var, m, v
```



<br />

<br />




## 7、总结：

> 1、Gradient Descent
>
> 权重更新： $w \leftarrow w - \eta*g$

> 2、Gradient Descent with momentum
>
> 累计梯度更新：$v \leftarrow \alpha v + (1 - \alpha)g$
>
> 权重更新： $w \leftarrow w - \eta*v$
>
> $\alpha$ 为动量参数，$v$累计梯度，$\eta$为学习率



> 3、Adagrad
>
> 累计平方梯度：$r \leftarrow r + g^2$
>
> 权重更新：$w \leftarrow w - \frac{\eta}{\sqrt {r + \delta}}*g$
>
> 其中，g为梯度，r为累积平方梯度(初始为0) ，$\eta$为学习率，$\delta$为小参数，避免分母为0




> 4、RMSProp
>
> 累计平方梯度：$r \leftarrow \lambda r + (1 - \lambda)g^2$
>
> 权重更新：$w \leftarrow w - \frac{\eta}{\sqrt {r + \delta}}*g$
>
> 其中，g为梯度，r为累积平方梯度(初始为0) ，$\eta$为学习率，$\delta$为小参数，避免分母为0




> 5、Adam
>
> 累计梯度：$v \leftarrow \alpha v + (1 - \alpha)g$
>
> 累计平方梯度：$r \leftarrow \lambda r + (1 - \lambda)g^2$
>
> 偏差校正：$\hat{v} = \frac{v}{1 - \alpha^t}$     
>
> 偏差校正：$\hat{r} = \frac{r}{1 - \lambda^t}$
>
> 权重更新：$w \leftarrow w - \frac{\eta}{\sqrt {\hat{r} + \delta}}*\hat{v}$
>
> 其中：
>
> - g为梯度，$\alpha$为动量参数，$v$为累积梯度，
> - $\lambda$为衰减系数，r为累积平方梯度(初始为0) ，
> - $\eta$为学习率，$\delta$为小参数，避免分母为0

