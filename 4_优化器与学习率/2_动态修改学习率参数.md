# 二、动态修改学习率参数

修改参数的方式可以通过修改参数optimizer.params_groups或新建optimizer。新建optimizer比较简单，optimizer 十分轻量级，所以开销很小。但是新的优化器会初始化动量等状态信息，这对于使用动量的优化器（momentum参数的sgd）可能会造成收敛中的震荡。所以，这一般采用修改参数optimizer.params_groups


查看优化器的参数，有8个参数

```python
print(optimizer.param_groups[0].keys())

输出：
dict_keys(['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov', 'maximize', 'foreach'])
```

用pycharm查看：
![在这里插入图片描述](https://img-blog.csdnimg.cn/47486113dceb40da86f7df1a2aa2a744.png =500x)
我们可以通过动态修改参数 "lr" 的值，来实现动态参数的调整：
比如，我们想要每 5个 epoch 修改一次学习率，改为之前学习率的 0.1 倍

```python
if epoch % 5 == 0:
    optimizer.param_groups[0]['lr'] *= 0.1
```

