# 一、交叉熵损失函数
交叉熵损失多用于 多分类函数，下面我们通过拆解交叉熵的公式来理解其作为损失函数的意义

假设我们在做一个  n分类的问题，模型预测的输出结果是 $[x_1,  x_2, x_3, ...., x_n]$
然后，我们选择交叉熵损失函数作为目标函数，通过反向传播调整模型的权重

nn.CrossEntropyLoss() 的公式为：
$$
\begin{aligned}
loss(x, class) 
&= -log(\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}})\\
&= -x_{[class]} + log(\sum_j e^{x_{j}})
\end{aligned}
$$
-  x 是预测结果，是一个向量 x=$[x_1,  x_2, x_3, ...., x_n]$，其元素个数和类别数一样多
-  class 表示这个样本的实际标签，比如，样本实际属于分类2，那么class=2
    $x_{[class]}$ 就是 $x_2$，就是取测试结果向量中的第二个元素，也就是取其真实分类对应的那个预测值


---

上面铺垫完了，接下来，我们来拆解公式，理解公式：


 1. 首先，交叉熵损失函数中包含了一个最基础的部分：$softmax(x_i) = \frac{e^{x_i}}{\sum_je^{x_{j}}}$
\
softmax 将分类的结果做了归一化：
	- 先经过 $e^{x}$ 的运算，将 $x$ 转换为非负数
	- 再通过公式 $\frac{e^{x_i}}{\sum_{j=0}^ne^{x_j}}$计算出该样本被分到分类$i$的概率，这里所有分类概率相加的总和等于1。
$\quad$

 2. 我们想要使预测结果中，真实分类的那个概率接近 100%。 我们取出真实分类的那个概率：
$\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}$，我们希望它的值是 100%
$\quad$

 3. 作为损失函数，后面需要参与求导。乘/除法 表达式求导比较麻烦，所以最好想办法转化成加/减法表达式。最自然的想法是取对数，把乘/除法转化为加/减法表达式。
$$log{\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}} = log{e^{x_{[class]}}} - log{\sum_je^{x_{j}}}$$
	- 由于对数单调增，那么求$\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}$的最大值的问题，可以转化为求$log{\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}}$的最大值的问题。
	- $\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}$ 的取值范围是 (0, 1)，最大值为1。 取对数之后，$log{\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}}$ 的取值范围为 $[-\infty, 0]$，最大值为0。
$\quad$


 4. 作为损失函数的意义是：当预测结果越==接近真实值==，==损失函数的值越接近于0==。
\
所以，我们把$log{\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}}$取反之后，$-log(\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}})$ 最小值为0。 就能保证当  $\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}$ 越接近于100%， $loss=-log(\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}})$ 越接近0。


 附上一张 $-log^x$ 的图
![在这里插入图片描述](https://img-blog.csdnimg.cn/f2327cc921684b0aac2a8e0da57a2eb8.png =300x)


---

# 二、nn.CrossEntropyLoss
pytorch 的交叉熵损失函数
```python
nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
```
如果设置了权重参数weight，则
$$
loss(x, class) = weight_{[class]}(-log(\frac{e^{x_{[class]}}}{\sum_je^{x_{j}}}))
$$
weigh 为每个类别的loss设置权值，常用于类别不均衡问题。weight必须是float类型的tensor，其长度要与类别C一致，即每一个类别都要设置weight


----

# 三、应用
假设有4张图片，或者说batch_ size=4。我们需要把这4张图片分类到5个类别上去，比如说：鸟，狗，猫，汽车，船
经过网络计算后，我们得到了预测结果：predict，size为[4, 5]
其真实标签为 label，size为 [4]
接下来使用 nn.CrossEntropyLoss()  计算 ==预测结果predict== 和 ==真实值label== 的交叉熵损失，可以
```python
import torch
import torch.nn as nn

# -----------------------------------------
# 定义数据: batch_size=4；  一共有5个分类
# label.size() : torch.Size([4])
# predict.size(): torch.Size([4, 5])
# -----------------------------------------
torch.manual_seed(100)
predict = torch.rand(4, 5)
label = torch.tensor([4, 3, 3, 2])
print(predict)
print(label)

# -----------------------------------------
# 直接调用函数 nn.CrossEntropyLoss() 计算 Loss
# -----------------------------------------
criterion = nn.CrossEntropyLoss()
loss = criterion(predict, label)
print(loss)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/0e7b87499a3e46f1a143b8253b5c7045.png =300x)




----
nn.CrossEntropyLoss() 可以拆解成如下3个步骤，或者说可以由如下3个操作替换，其运算结果一毛一样：
 1. softmax：对每张图片的分类结果做softmax， [softmax详细介绍](https://editor.csdn.net/md/?articleId=125253161)
 2. log：对上面的结果 取log  
（步骤1 和 步骤2 可以合并为 nn.logSoftmax() ）
 3. NLL：nn.NLLLoss(a, b) 的操作是从a 中取出b对应的那个值(b中存的是 index值)，再去掉负号（取反），然后求和取均值

```python
import torch
import torch.nn as nn

torch.manual_seed(100)
predict = torch.rand(4, 5)
label = torch.tensor([4, 3, 3, 2])

softmax = nn.Softmax(dim=1)
nll = nn.NLLLoss()

temp1 = softmax(predict)
temp2 = torch.log(temp1)
output = nll(temp2, label)
print(output)   # tensor(1.5230)
```

纯手撸版本

```python
import torch

torch.manual_seed(100)
predict = torch.rand(4, 5)
label = torch.tensor([4, 3, 3, 2])

# softmax
temp1 = torch.exp(predict) / torch.sum(torch.exp(predict), dim=1, keepdim=True)

# log
temp2 = torch.log(temp1)

# nll
temp3 = torch.gather(temp2, dim=1, index=label.view(-1, 1))
temp4 = -temp3
output = torch.mean(temp4)

print(output)    # tensor(1.5230)
```