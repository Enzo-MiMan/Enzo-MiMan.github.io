## **不同层设置 不同学习率**



**1、在迁移学习的时候，你可能需要用到如下训练技巧**

（1）将 预训练好的 backbone 的 参数学习率设置为较小值

（2）backbone 之外的部分 (新增的部分，一般为分类头、检测头，等)，设置为较大的学习率。



$\quad$



**2、学习率设置方式**

在定义优化器的时候，用 list 将参数设置为不同的组，每个组( list 中的每个元素 )用字典表示，在字典中指明 参数组、该组的学习率

```python
optimizer = optim.SGD([
    {'params': params_group_1, 'lr': 0.001},
    {'params': params_group_2, 'lr': 0.0005}]) 
```

** params_group_1 和 params_group_2 可以是任何实现了 \_\_iter\__() 方法的对象，例如 list、tuple 



举例

```python
from collections import OrderedDict
import torch.nn as nn
import torch.optim as optim

net = nn.Sequential(OrderedDict([
    ("linear1", nn.Linear(10, 20)),
    ("linear2", nn.Linear(20, 30)),
    ("linear3", nn.Linear(30, 40))]))


linear3_params = list(map(id, net.linear3.parameters()))
base_params = filter(lambda p: id(p) not in linear3_params, net.parameters())

optimizer = optim.SGD([
    {'params': base_params},   # 未指定学习率的，使用默认学习率 0.001
    {'params': net.linear3.parameters(), 'lr': 0.0005}],
    lr=0.001, momentum=0.9)


# print(optimizer)
print(optimizer.param_groups[0]['lr'])
print(optimizer.param_groups[1]['lr']) 
```





